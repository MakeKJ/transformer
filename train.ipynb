{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db9a65f",
   "metadata": {},
   "source": [
    "Training a character level Transformer model built with pytorch on the first Harry Potter book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78053169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Importin own modules and functions\n",
    "from models import Transformer\n",
    "from load_hp_data import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0885996",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28cefacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw dataset in characters:  439478\n",
      "Characters present in the raw dataset:  \n",
      " !'()*,-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz–—‘’“”…\n",
      "Vocabulary size of the raw dataset:  82\n",
      "\n",
      "\n",
      "length of dataset after augmentation in characters:  435847\n",
      "Characters present in the augmented dataset:  \n",
      " !\"',-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz…\n",
      "Vocabulary size of the augmented dataset:  64\n",
      "\n",
      "\n",
      "torch.Size([435847]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "data_file = 'hp_dataset/01 Harry Potter and the Sorcerers Stone.txt'\n",
    "text, vocab_size, encode, decode = load_data(data_file)\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "\n",
    "# # Write the augmented text to a file\n",
    "# with open('hp_dataset/augmented_hp_1.txt', 'w') as f:\n",
    "#     f.write(decode(data.tolist()))\n",
    "\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0e790",
   "metadata": {},
   "source": [
    "Example from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853578b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\n",
      "\n",
      "\n",
      "tensor([23,  1, 54,  7,  1, 37, 50, 40,  1, 23, 54, 55,  7,  1, 14, 57, 54, 55,\n",
      "        48, 41, 61,  5,  1, 51, 42,  1, 50, 57, 49, 38, 41, 54,  1, 42, 51, 57,\n",
      "        54,  5,  1, 26, 54, 45, 58, 41, 56,  1, 14, 54, 45, 58, 41,  5,  1, 59,\n",
      "        41, 54, 41,  1, 52, 54, 51, 57, 40,  1, 56, 51,  1, 55, 37, 61,  1, 56,\n",
      "        44, 37, 56,  1, 56, 44, 41, 61,  1, 59, 41, 54, 41,  1, 52, 41, 54, 42,\n",
      "        41, 39, 56, 48, 61,  1, 50, 51, 54, 49, 37, 48,  5,  1, 56, 44, 37, 50,\n",
      "        47,  1, 61, 51, 57,  1, 58, 41, 54, 61,  1, 49, 57, 39, 44,  7],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(text[:124])\n",
    "print('\\n')\n",
    "print(data[:124])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a20e8",
   "metadata": {},
   "source": [
    "Simple chronological split into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b188f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf167068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size, context_len):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_len, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_len] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_len+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067c16d",
   "metadata": {},
   "source": [
    "Initializing the model and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cbd5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = 256\n",
    "embed_dim = 384\n",
    "n_heads = 6\n",
    "n_blocks = 6\n",
    "batch_size = 64\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 3e-4\n",
    "\n",
    "max_iters = 5000\n",
    "eval_interval = 250\n",
    "eval_iters = 50\n",
    "\n",
    "model = Transformer(vocab_size, context_len, embed_dim, n_heads, n_blocks, dropout_rate)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# Default div_factor 25 and final_div_factor 1000\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, total_steps=max_iters, pct_start=0.1, anneal_strategy='cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b7e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\" Helper function to estimate loss on train and val splits. \"\"\"\n",
    "    out = {}\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size, context_len)\n",
    "            logits = model(X)\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            Y = Y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Set model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e09b6b5",
   "metadata": {},
   "source": [
    "Training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_num in range(max_iters):\n",
    "\n",
    "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Get a batch of data\n",
    "    xb, yb = get_batch('train', batch_size, context_len)\n",
    "\n",
    "    # Forward pass and loss calculation\n",
    "    logits = model(xb)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    yb = yb.view(B*T)\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n",
    "\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"\\nTraining finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0d4b2",
   "metadata": {},
   "source": [
    "Save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAVE_PATH = 'saved_models/model.pth'\n",
    "os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
