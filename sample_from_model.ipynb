{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c044dec2",
   "metadata": {},
   "source": [
    "Sampling from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29a1b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "torch.manual_seed(123)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from models import Transformer\n",
    "from load_hp_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dab1520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of raw dataset in characters:  439478\n",
      "Characters present in the raw dataset:  \n",
      " !'()*,-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz–—‘’“”…\n",
      "Vocabulary size of the raw dataset:  82\n",
      "\n",
      "\n",
      "length of dataset after augmentation in characters:  435847\n",
      "Characters present in the augmented dataset:  \n",
      " !\"',-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz…\n",
      "Vocabulary size of the augmented dataset:  64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_file = 'hp_dataset/01 Harry Potter and the Sorcerers Stone.txt'\n",
    "text, vocab_size, encode, decode = load_data(data_file)\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be470a2",
   "metadata": {},
   "source": [
    "Load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f099fec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding): Embedding(64, 384)\n",
       "  (position_embedding): Embedding(256, 384)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (attention_heads): ModuleList(\n",
       "        (0-5): 6 x SingleHeadAttention(\n",
       "          (k): Linear(in_features=384, out_features=64, bias=False)\n",
       "          (q): Linear(in_features=384, out_features=64, bias=False)\n",
       "          (v): Linear(in_features=384, out_features=64, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (attention_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "      )\n",
       "      (ffn_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (language_linear): Linear(in_features=384, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_len = 256\n",
    "embed_dim = 384\n",
    "n_heads = 6\n",
    "n_blocks = 6\n",
    "\n",
    "loaded_model = Transformer(vocab_size, context_len, embed_dim, n_heads, n_blocks)\n",
    "\n",
    "SAVE_PATH = 'saved_models/hp_model_cl_256_ed_384_nh_6_nb_6_bs_64_dr_0.2_lr_3e-4_max_iters_5000.pth'\n",
    "# loaded_model.load_state_dict(torch.load(SAVE_PATH))\n",
    "loaded_model.load_state_dict(torch.load(SAVE_PATH, map_location=device))\n",
    "loaded_model.eval()\n",
    "loaded_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76cd401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_len, decode_func):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop the context to at most context_len tokens\n",
    "            idx_cond = idx[:, -context_len:]\n",
    "\n",
    "            logits = model(idx_cond)\n",
    "            \n",
    "            # Focus only on the logit for the last time step\n",
    "            logits = logits[:, -1, :] # Becomes (B, C)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # Real-time printing: decode the token and print immediately\n",
    "            token_id = idx_next.item()\n",
    "            print(decode_func([token_id]), end=\"\", flush=True)\n",
    "            \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62426924",
   "metadata": {},
   "source": [
    "Sampling from the model with blank context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0bf4111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating from a blank context ---\n",
      "\"Brewill time three-heak,\" said great on the chance into the air.\n",
      "\"Ah, Malfoy!\" shouted. \"Can I can't stood get platformward Harry.\"\n",
      "\"I'm not must to lose without anyway, every now,\" said Hagrid, \"but\n",
      "--- Generation Complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Generating from a blank context ---\")\n",
    "\n",
    "# Ensure device is set (from your previous code)\n",
    "device = 'cpu' \n",
    "\n",
    "start_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "tokens_to_generate = 200\n",
    "\n",
    "# We pass 'decode' as an argument so the function can use it\n",
    "generated_ids = generate(\n",
    "    model=loaded_model,\n",
    "    idx=start_context,\n",
    "    max_new_tokens=tokens_to_generate,\n",
    "    context_len=256,\n",
    "    decode_func=decode # Passing your existing decode function here\n",
    ")\n",
    "\n",
    "print(\"\\n--- Generation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
